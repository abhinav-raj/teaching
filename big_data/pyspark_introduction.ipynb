{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark_introduction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMUDOU4R0w6g",
        "colab_type": "text"
      },
      "source": [
        "# PySpark Introduction\n",
        "\n",
        "Upload this jupyter notebook to Google drive, and open this tutorial with Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9kZ5EW9qsxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install the dependencies:\n",
        "%env spark_version=2.4.4\n",
        "%env hadoop_version=2.7\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-${spark_version}/spark-${spark_version}-bin-hadoop${hadoop_version}.tgz\n",
        "!tar xf spark-${spark_version}-bin-hadoop${hadoop_version}.tgz\n",
        "!pip install -q findspark\n",
        "!python --version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZlOhOZGreFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set environment\n",
        "import os\n",
        "\n",
        "current_directory = os.getcwd()\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"{}/spark-{}-bin-hadoop{}\".format(current_directory, os.environ[\"spark_version\"], os.environ[\"hadoop_version\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qCjjRcyuJqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install pyspark\n",
        "!pip install pyspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0SU5vJSteSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init() # os.environ[\"SPARK_HOME\"]\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.rdd import RDD\n",
        "\n",
        "\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "# create SparkConf\n",
        "conf = SparkConf().setAppName('pyspark-app').setMaster('local[*]')\n",
        "# create SparkSession instance\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
        "spark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxmoiAx3kq5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc = spark.sparkContext\n",
        "sc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwIC2YSscOJD",
        "colab_type": "text"
      },
      "source": [
        "# Part A: RDD\n",
        "\n",
        "RDD stands for Resilient Distributed Dataset, these are the elements that run and operate on multiple nodes to do parallel processing on a cluster. \n",
        "\n",
        "* Distributed: RDD are distributed in nature.\n",
        "* Fault tolerant: In case of any failure, they recover automatically.\n",
        "* Immutable in nature : We can create RDD once but can’t change it. And we can transform a RDD after applying transformations.\n",
        "* Lazy Evaluations: Which means that a task is not executed until an action is performed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsE1rr-3ctsS",
        "colab_type": "text"
      },
      "source": [
        "There are two ways to create RDDs: parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKJtP_2edBIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download data\n",
        "\n",
        "import requests\n",
        "\n",
        "url = \"https://github.com/liuhoward/teaching/raw/master/big_data/\"\n",
        "blackfriday_file = \"BlackFriday_lite.csv\"\n",
        "\n",
        "r = requests.get(url + blackfriday_file)\n",
        "open(blackfriday_file, 'wb').write(r.content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kgF2uQFzowc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read from local file\n",
        "\n",
        "RDDread = sc.textFile (f\"file:///{current_directory}/{blackfriday_file}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxoNekHXzozH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show 3 elements\n",
        "RDDread.take(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlQPGacJzo5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create RDD from list\n",
        "\n",
        "sentences = sc.parallelize(\n",
        "   [\"scala is easier than java\", \n",
        "   \"python is easier than java\", \n",
        "   \"hadoop is good\", \n",
        "   \"spark is easier than hadoop\", \n",
        "   \"spark vs hadoop\", \n",
        "   \"pyspark is python api for spark\",\n",
        "   \"pyspark and spark\"]\n",
        ")\n",
        "\n",
        "nums = sc.parallelize([1,2,3,4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb94cDe0zo8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(sentences.collect())\n",
        "print(nums.collect())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXPNoQ4Pfsw5",
        "colab_type": "text"
      },
      "source": [
        "RDDs have two sets of parallel operations:\n",
        "\n",
        "* Transformation − These are the operations, which are applied on a RDD to create a new RDD. They return pointers to new RDDs without computing them, it rather waits for an action to compute itself. Such as\n",
        "map()\n",
        "flatMap()\n",
        "filter()\n",
        "sample()\n",
        "union()\n",
        "intersection()\n",
        "distinct()\n",
        "join()\n",
        "\n",
        "* Action − These are the operations that are applied on RDD, which instructs Spark to perform computation and send the result back to the driver. The collect() funcion is an operation which retrieves all elements of the distributed RDD to the driver. Such as\n",
        "reduce()\n",
        "reduceByKey()\n",
        "collect()\n",
        "count() \n",
        "first()  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "287WytshhtSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# count()\n",
        "# Number of elements in the RDD is returned.\n",
        "sentences.count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBBH1UM1cDFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Action: max, min, sum, variance and stdev\n",
        "\n",
        "nums.max(),nums.min(), nums.sum(),nums.variance(),nums.stdev() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1ULozAWhtWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filter(f)\n",
        "# A new RDD is returned containing the elements, which satisfies the function inside the filter. \n",
        "\n",
        "spark_sentences = sentences.filter(lambda x: 'spark' in x)\n",
        "spark_sentences.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjftcxSUhtcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# map(f)\n",
        "# A new RDD is returned by applying a function to each element in the RDD.\n",
        "\n",
        "# map sentence -> length\n",
        "sentence_length = sentences.map(lambda x: len(x))\n",
        "print(sentence_length.collect())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o76VXmGwhtgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# map sentence -> (sentence, length)\n",
        "sentence_length = sentences.map(lambda x: (x, len(x)))\n",
        "print(sentence_length.collect())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qAhf06dhtil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split each sentence by space\n",
        "word_list = sentences.map(lambda x: x.split(' '))\n",
        "print(word_list.collect())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcuiITCDoXa9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# flatMap()\n",
        "# This transformation apply changes to each line same as map \n",
        "# but the return is not a iterable of iterables but it is only an iterable holding entire RDD contents.\n",
        "\n",
        "word_list1 = sentences.flatMap(lambda x: x.split(' '))\n",
        "print(word_list1.collect())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocy7SnRKoXdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Union()\n",
        "# Union is basically used to merge two RDDs together if they have the same structure.\n",
        "\n",
        "A_marks = [(\"physics\",85),(\"maths\",75),(\"chemistry\",95)]\n",
        "B_marks = [(\"physics\",65),(\"maths\",45),(\"chemistry\",85)]\n",
        "\n",
        "A = sc.parallelize(A_marks)\n",
        "B = sc.parallelize(B_marks)\n",
        "\n",
        "A.union(B).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpeoy2zDoXfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# join()\n",
        "# This transformation joins two RDDs based on a common key.\n",
        "\n",
        "A_marks = [(\"physics\",85),(\"maths\",75),(\"chemistry\",95)]\n",
        "B_marks = [(\"physics\",65),(\"maths\",45),(\"chemistry\",85)]\n",
        "\n",
        "A = sc.parallelize(A_marks)\n",
        "B = sc.parallelize(B_marks)\n",
        "\n",
        "A.join(B).collect()\n",
        "\n",
        "# what if we change \"maths\" in B_marks to \"maths2\"?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6SS8RjSoXhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# intersection() Transformation in Spark\n",
        "# Intersection gives you the common terms or objects from the two RDDS.\n",
        "\n",
        "A_words = ['scala', 'is', 'easier', 'than', 'java']\n",
        "B_words = ['python', 'is', 'easier', 'than', 'java']\n",
        "\n",
        "A = sc.parallelize(A_words)\n",
        "B = sc.parallelize(B_words)\n",
        "\n",
        "AB_intersect = A.intersection(B)\n",
        "AB_intersect.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLHdOGPPoXnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# distinct()\n",
        "\n",
        "A_words = ['scala', 'is', 'easier', 'than', 'java', 'python', 'is', 'easier', 'than', 'java']\n",
        "\n",
        "A = sc.parallelize(A_words)\n",
        "\n",
        "distinct_A = A.distinct()\n",
        "distinct_A.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn9-qK5hoXln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reduce()\n",
        "# The .reduce(function) transformation reduces all elements of the RDD into one using a specific method.\n",
        "\n",
        "nums = sc.parallelize([1,2,3,4])\n",
        "\n",
        "adding = nums.reduce(lambda x,y: x+y)\n",
        "print(adding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7DjcUZZoXkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reduceByKey()\n",
        "# The .reduceByKey() method works in a similar way to the .reduce(), \n",
        "# but it performs a reduction on a key-by-key basis.\n",
        "\n",
        "pairs = [('a', 3), ('d', 4), ('a', 6)]\n",
        "pairs_rdd = sc.parallelize(pairs)\n",
        "\n",
        "new_pairs_count = pairs_rdd.reduceByKey(lambda x,y: x+y)\n",
        "new_pairs_count.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLCK_nLVtrcb",
        "colab_type": "text"
      },
      "source": [
        "### word count using RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r0u1eWUhtlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = sc.parallelize(\n",
        "   [\"scala is easier than java\", \n",
        "   \"python is easier than java\", \n",
        "   \"hadoop is good\", \n",
        "   \"spark is easier than hadoop\", \n",
        "   \"spark vs hadoop\", \n",
        "   \"pyspark is python api for spark\",\n",
        "   \"pyspark and spark\"]\n",
        ")\n",
        "\n",
        "counts = sentences.flatMap(lambda line: line.split(\" \")) \\\n",
        "             .map(lambda word: (word, 1)) \\\n",
        "             .reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "counts.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq38gqDavPlR",
        "colab_type": "text"
      },
      "source": [
        "# Part B: SparkSQL (Dataframe)\n",
        "\n",
        "SparkSQL, Spark's interface for working with structured data. From Spark 2.0 and forward, this is the preferred way of implementing Spark code, as it contains all of the latest optimisations.\n",
        "\n",
        "PySpark benefits a lot from SparkSQL, as there is performance parity between Scala, Java, Python and R interfaces for Spark which use the same optimizer.\n",
        "\n",
        "A **Dataset** is a distributed collection of data which provides the benefits of RDDs (strong typing, ability to use lambda functions) with the benefits of SparkSQL's optimized execution engine.\n",
        "\n",
        "A **DataFrame** is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database, or a data frame in Python/R. Conceptually, a DataFrame is a Dataset of Rows.\n",
        "\n",
        "As with **RDDs**, applications can create DataFrames from an existing RDD, a Hive table or from Spark data sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWhgd5_idE6Y",
        "colab_type": "text"
      },
      "source": [
        "In Apache Spark, a DataFrame is a distributed collection of rows under named columns. In simple terms, it is same as a table in relational database or an Excel sheet with Column headers. It also shares some common characteristics with RDD:\n",
        "\n",
        "* Distributed: RDD and DataFrame both are distributed in nature.\n",
        "* Immutable in nature : We can create DataFrame / RDD once but can’t change it. And we can transform a DataFrame / RDD  after applying transformations.\n",
        "* Lazy Evaluations: Which means that a task is not executed until an action is performed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr8tOlXseo7w",
        "colab_type": "text"
      },
      "source": [
        "Advantages of DataFrames:\n",
        "\n",
        "* DataFrames are designed for processing large collection of structured or semi-structured data.\n",
        "* Observations in Spark DataFrame are organised under named columns, which helps Apache Spark to understand the schema of a DataFrame. This helps Spark optimize execution plan on these queries.\n",
        "* DataFrame in Apache Spark has the ability to handle petabytes of data.\n",
        "* DataFrame has a support for wide range of data format and sources.\n",
        "* It has API support for different languages like Python, R, Scala, Java."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m0QJH2ZlnwN",
        "colab_type": "text"
      },
      "source": [
        "#### create Dataframe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlisL3qDuIWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create Dataframe from RDD\n",
        "\n",
        "from pyspark.sql import Row\n",
        "\n",
        "data = [('Anna',25),('Jack',22),('Tom',20),('Andy',26)]\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "people = spark.createDataFrame(rdd, [\"name\", \"age\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uykbw3gKuIal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"type: {type(people)}\")\n",
        "people.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct5OqBRnvqO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Q: what is the relationship between Dataframe and RDD?\n",
        "\n",
        "people.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMSawTu5uYC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "# define schema to strict type\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True)])\n",
        "\n",
        "people = spark.createDataFrame(rdd, schema=schema)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukKY_9TQuIrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create dataframe from Pandas dataframe\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Enable Arrow-based columnar data transfers\n",
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
        "\n",
        "# Generate a Pandas DataFrame\n",
        "pdf = pd.DataFrame(np.random.rand(5, 3), columns=[\"A\", \"B\", \"C\"])\n",
        "pdf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y-CQQ5y5zTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
        "df = spark.createDataFrame(pdf)\n",
        "df.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w0iAuWM56Yg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
        "result_pdf = df.toPandas()\n",
        "result_pdf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwXxupbluIjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create dataframe from json file\n",
        "\n",
        "# download json data\n",
        "import requests\n",
        "\n",
        "url = \"https://github.com/liuhoward/teaching/raw/master/big_data/\"\n",
        "json_file = \"people.json\"\n",
        "\n",
        "r = requests.get(url + json_file)\n",
        "open(json_file, 'wb').write(r.content)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOWNQCU33kqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create dataframe from json file\n",
        "\n",
        "# define schema\n",
        "data_schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", StringType(), True)])\n",
        "\n",
        "people = spark.read.json(json_file, schema=data_schema)\n",
        "\n",
        "people.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrbCP-rH8Ryb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create dataframe from csv file\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# define schema\n",
        "data_schema = StructType([\n",
        "    StructField(\"category\", StringType()),\n",
        "    StructField(\"text\", StringType())])\n",
        "\n",
        "transactions = spark.read.csv(blackfriday_file, sep=',', header=True, inferSchema=True)\n",
        "\n",
        "transactions.show(n=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzxL-iucy6BB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show schema details, see datatype of columns\n",
        "transactions.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k_iU72VzCuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show first n observation\n",
        "transactions.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2sUqgVh-xL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transactions.take(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wWcG4AEzCww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Count the number of rows\n",
        "\n",
        "transactions.count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-5Shx1pzC8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How many columns do we have\n",
        "\n",
        "len(transactions.columns), transactions.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN9huyGxzDAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the summary statistics (mean, standard deviance, min ,max, count) of numerical columns\n",
        "\n",
        "people.describe().show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x42RWZczDQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the summary statistics for age column\n",
        "\n",
        "people.describe('age').show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqrkvRICzDTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select column(s) from the DataFrame\n",
        "\n",
        "transactions.select('User_ID','Age').show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5YBNCfjzC5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# find the number of distinct product\n",
        "\n",
        "transactions.select('Product_ID').distinct().count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNSA9XtLuIpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate pair wise frequency of categorical columns\n",
        "\n",
        "transactions.crosstab('Age', 'Gender').show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLOZ6mYFuIna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the DataFrame which won’t have duplicate rows\n",
        "\n",
        "transactions.select('Age','Gender').dropDuplicates().show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F8OIk4uuIhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  drop the all rows with null value\n",
        "transactions.dropna().count()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnNYR6-zuIc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fill '-1' inplace of null values \n",
        "\n",
        "transactions.fillna(\"-1\").show(5)\n",
        "\n",
        "#transactions.fillna(-1).show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF-3SzXQ_Yji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filter the rows which have Purchase more than 15000\n",
        "\n",
        "transactions.filter(transactions.Purchase > 15000).select(\"User_ID\", \"Product_ID\", \"Purchase\").show(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k67X2SsWGgsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# grou py age, count transactions in each age group\n",
        "\n",
        "transactions.groupby('Age').count().show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-MaGqnSGgk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# grou py age, find the mean of each age group, use aggregation\n",
        "\n",
        "# We can also apply sum, min, max, count with groupby\n",
        "\n",
        "transactions.groupby('Age').agg({'Purchase': 'mean'}).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPy0dzrcGgvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sort the DataFrame based on column(s)\n",
        "\n",
        "transactions.orderBy(transactions.Purchase.desc()).show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kumvYz9EGgyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add the new column in DataFrame\n",
        "# withColumn():\n",
        "#     Column name which we want add /replace.\n",
        "#     Expression on column.\n",
        "\n",
        "# ‘Purchase_new’ is calculated by dviding Purchase column by 2\n",
        "\n",
        "transactions.withColumn('Purchase_new', transactions.Purchase /2.0).select('User_ID','Purchase','Purchase_new').show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOHcBdrWNuu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change User_ID from IntegerType to StringType\n",
        "\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "transactions.withColumn(\"User_ID_Str\", transactions[\"User_ID\"].cast(StringType())).printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx_NHm_8GgiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stop\n",
        "spark.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIAa4NRefMGB",
        "colab_type": "text"
      },
      "source": [
        "## Reference\n",
        "\n",
        "[1] https://spark.apache.org/docs/latest/api/python/pyspark.sql.html  \n",
        "[2] https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/  \n",
        "[3] https://www.guru99.com/pyspark-tutorial.html  \n",
        "[4] https://github.com/andfanilo/pyspark-tutorial  \n",
        "[5] https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html  \n",
        "[6] https://data-flair.training/blogs/spark-rdd-tutorial/  \n",
        "[7] https://spark.apache.org/docs/latest/sql-getting-started.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMVswQGif_v-",
        "colab_type": "text"
      },
      "source": [
        "# End"
      ]
    }
  ]
}