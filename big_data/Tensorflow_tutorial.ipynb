{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNHXqJjGpSZa",
        "colab_type": "text"
      },
      "source": [
        "# Tensorflow Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0meHP4ljY4q3",
        "colab_type": "text"
      },
      "source": [
        "In menu **Runtime**, chose **change runtime type**, in **Hardware accelerator** choose **GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlnYEPuwemT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U tensorflow-gpu tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJzLAeH-pWwv",
        "colab_type": "text"
      },
      "source": [
        "## Multiple layer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KImCI_sDpIss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7NAbSZiaoJ4z"
      },
      "source": [
        "Load and prepare the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). Convert the samples from integers to floating-point numbers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7ioWVRypoPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YvUNj-3rE0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"x type: {type(x_train)}\")\n",
        "print(f\"x dtype: {x_train.dtype}\")\n",
        "print(f\"y type: {type(y_train)}\")\n",
        "print(f\"x shape: {np.shape(x_train)}\")\n",
        "print(f\"y shape: {np.shape(y_train)}\")\n",
        "print(f\"y unique: {np.unique(y_train)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2398991rE3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "fig = plt.figure(figsize=(9,9))\n",
        "# randomly show 9 images from x_train\n",
        "for i in range(9):\n",
        "    ax = fig.add_subplot(3,3,i+1)\n",
        "    img = x_train[random.randint(0,60000)]\n",
        "    plt.imshow(img)\n",
        "    plt.colorbar()\n",
        "    plt.grid(False)\n",
        "    plt.axis('off')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyl7-I9trAIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalize images\n",
        "x_train = x_train.astype(np.float32)\n",
        "x_test = x_test.astype(np.float32)\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXzkiFVutvpc",
        "colab_type": "text"
      },
      "source": [
        "Build the tf.keras.Sequential model by stacking layers. Choose an optimizer and loss function for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl3-BJSAtulY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),  # flatten image as 1-D array\n",
        "  tf.keras.layers.Dense(128, activation='relu'),  # fully-connected layer\n",
        "  tf.keras.layers.Dropout(0.2),                   # dropout to avoid overfitting\n",
        "  tf.keras.layers.Dense(10, activation='softmax') # output layer with 10 classes\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',                   \n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUMwai1kq0Eo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHXSGXkxuT5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate\n",
        "test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=2)\n",
        "\n",
        "print(f\"test_loss: {test_loss}, test_accuracy: {test_acc}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9za5eYuq22g",
        "colab_type": "text"
      },
      "source": [
        "The image classifier is now trained to ~98% accuracy on this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfmgRTi-uog1",
        "colab_type": "text"
      },
      "source": [
        "## Convolutional Neural Networks (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNPGNWGKuu1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load dataset\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "# normalize images\n",
        "x_train = x_train.astype(np.float32)\n",
        "x_test = x_test.astype(np.float32)\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icOv3_L92Ks-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Reshaping the array to 4-dims so that it can work with the tf.keras API\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\n",
        "# width x height x channel\n",
        "input_shape = (28, 28, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCkdbZla1THH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the required Keras modules containing model and layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
        "# Creating a Sequential Model and adding the layers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(8, kernel_size=(3,3), input_shape=input_shape))   # 8 filters\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(16, kernel_size=(3,3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "#model.add(Conv2D(64, kernel_size=(3,3)))\n",
        "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10,activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEBhCZdH1rAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',                   \n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYA-_ygY1vjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training\n",
        "\n",
        "epochs = 10\n",
        "history = model.fit(x_train, y_train, epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c8iYn3e4XZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLN_safN1zGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate\n",
        "test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=2)\n",
        "\n",
        "print(f\"test_loss: {test_loss}, test_accuracy: {test_acc}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLU9b4pt6GUd",
        "colab_type": "text"
      },
      "source": [
        "## Long short term memory (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pRmMubr0jrE2"
      },
      "source": [
        "\n",
        "The IMDB large movie review dataset is a *binary classification* dataset—all the reviews have either a *positive* or *negative* sentiment.\n",
        "\n",
        "Download the dataset using [TFDS](https://www.tensorflow.org/datasets).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhXgawnHeTNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8TshzRk3wkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "(train_data, test_data), info = tfds.load(\n",
        "    # Use the version pre-encoded with an ~8k vocabulary.\n",
        "    'imdb_reviews/subwords8k', \n",
        "    # Return the train/test datasets as a tuple.\n",
        "    split = (tfds.Split.TRAIN, tfds.Split.TEST),\n",
        "    # Return (example, label) pairs from the dataset (instead of a dictionary).\n",
        "    as_supervised=True,\n",
        "    # Also return the `info` structure. \n",
        "    with_info=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecLmmaYpVFMG",
        "colab_type": "text"
      },
      "source": [
        "The dataset info includes the encoder (a tfds.features.text.SubwordTextEncoder)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH26TkwYVAP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = info.features['text'].encoder\n",
        "\n",
        "print ('Vocabulary size: {}'.format(encoder.vocab_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RppYQ94NVWfu",
        "colab_type": "text"
      },
      "source": [
        "This text encoder will reversibly encode any string, falling back to byte-encoding if necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoGRXRi1VZEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_string = 'Hello TensorFlow.'\n",
        "\n",
        "encoded_string = encoder.encode(sample_string)\n",
        "print ('Encoded string is {}'.format(encoded_string))\n",
        "\n",
        "original_string = encoder.decode(encoded_string)\n",
        "print ('The original string: \"{}\"'.format(original_string))\n",
        "\n",
        "assert original_string == sample_string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2etIsZSQerrB",
        "colab_type": "text"
      },
      "source": [
        "The encoder encodes the string by breaking it into subwords or characters if the word is not in its dictionary. So the more a string resembles the dataset, the shorter the encoded representation will be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j3213inVj5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for index in encoded_string:\n",
        "  print ('{} ----> {}'.format(index, encoder.decode([index])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq0rjoMYe0C2",
        "colab_type": "text"
      },
      "source": [
        "Let's take a moment to understand the format of the data. The dataset comes preprocessed: each example is an array of integers representing the words of the movie review.\n",
        "\n",
        "The text of reviews have been converted to integers, where each integer represents a specific word-piece in the dictionary.\n",
        "\n",
        "Each label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review.\n",
        "\n",
        "Here's what the first review looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDl307Q6e1V8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for train_example, train_label in train_data.take(1):\n",
        "  print('Encoded text:', train_example[:10].numpy())\n",
        "  print('Label:', train_label.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIsa2TXAfTdh",
        "colab_type": "text"
      },
      "source": [
        "The info structure contains the encoder/decoder. The encoder can be used to recover the original text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meHZffFifT1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder.decode(train_example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPPlLhUdWIYE",
        "colab_type": "text"
      },
      "source": [
        "You will want to create batches of training data for your model. The reviews are all different lengths, so use padded_batch to zero pad the sequences while batching:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTZWtY4aWHQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_batches = (\n",
        "    train_data\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .padded_batch(BATCH_SIZE, train_data.output_shapes))\n",
        "\n",
        "test_batches = (\n",
        "    test_data\n",
        "    .padded_batch(BATCH_SIZE, train_data.output_shapes))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwhjvnscfo4l",
        "colab_type": "text"
      },
      "source": [
        "Each batch will have a shape of (batch_size, sequence_length) because the padding is dynaimic each batch will have a different langth:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgZYYKKbfq9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for example_batch, label_batch in train_batches.take(2):\n",
        "  print(\"Batch shape:\", example_batch.shape)\n",
        "  print(\"label shape:\", label_batch.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4hAzEXeWQy-",
        "colab_type": "text"
      },
      "source": [
        "Build a tf.keras.Sequential model and start with an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n",
        "\n",
        "This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a tf.keras.layers.Dense layer.\n",
        "\n",
        "A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input—and then to the next.\n",
        "\n",
        "The tf.keras.layers.Bidirectional wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the output. This helps the RNN to learn long range dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brynA5a1WQQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(encoder.vocab_size, 32),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16,  return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the Keras model to configure the training process:\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12h65tXjWZIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 5\n",
        "\n",
        "history = model.fit(train_batches, epochs=epochs,\n",
        "                    validation_data=test_batches, \n",
        "                    validation_steps=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgxx7-ECaYRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhRHdFBRWeXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_batches)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiGt1A82cJmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_pred_text = ('The movie was cool. The animation and the graphics '\n",
        "                    'were out of this world. I would recommend this movie.')\n",
        "\n",
        "encoded_sample_pred_text = encoder.encode(sample_pred_text)\n",
        "\n",
        "encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n",
        "predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n",
        "print(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}