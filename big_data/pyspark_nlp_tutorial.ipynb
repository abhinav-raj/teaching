{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMUDOU4R0w6g",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark NLP Tutorial\n",
    "\n",
    "Upload this jupyter notebook to Google drive, and open this tutorial with Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V9kZ5EW9qsxQ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# install the dependencies:\n",
    "%env spark_version=2.4.4\n",
    "%env hadoop_version=2.7\n",
    "\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://www-us.apache.org/dist/spark/spark-${spark_version}/spark-${spark_version}-bin-hadoop${hadoop_version}.tgz\n",
    "!tar xf spark-${spark_version}-bin-hadoop${hadoop_version}.tgz\n",
    "\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZlOhOZGreFl",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# set environment\n",
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"{}/spark-{}-bin-hadoop{}\".format(current_directory, os.environ[\"spark_version\"], os.environ[\"hadoop_version\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1qCjjRcyuJqY",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# install findspark pyspark\n",
    "!pip install findspark\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O0SU5vJSteSA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() # os.environ[\"SPARK_HOME\"]\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName('pyspark-nlp-app').setMaster(\"local[*]\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EF-3SzXQ_Yji",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# download data\n",
    "import requests\n",
    "\n",
    "url = \"https://github.com/liuhoward/teaching/raw/master/big_data/smsspam/\"\n",
    "train_file = \"SMSSpamCollection.train\"\n",
    "test_file = \"SMSSpamCollection.test\"\n",
    "\n",
    "r = requests.get(url + train_file)\n",
    "open(train_file, 'wb').write(r.content)\n",
    "\n",
    "r = requests.get(url + test_file)\n",
    "open(test_file, 'wb').write(r.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jEY5lH8NthSf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load train data\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# define schema\n",
    "data_schema = StructType([\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"text\", StringType())])\n",
    "\n",
    "# read train csv file\n",
    "train_data = spark.read.csv(train_file, schema=data_schema, sep='\\t', header=None)\n",
    "print(type(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I6YfHbtaxpU9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_data.printSchema() # print detail schema of data\n",
    "train_data.show(n=5, truncate=True) # show top 5 rows\n",
    "train_data.count()  # number of examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X1qdZ3zgAab2",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# lower case\n",
    "\n",
    "from pyspark.sql.functions import lower, col\n",
    "\n",
    "lower_train = train_data.select('category', lower(col('text')).alias('text'))\n",
    "\n",
    "lower_train.show(n=5, truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bzSMr3Bt1oOx",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "#tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "tokenized_train = tokenizer.transform(lower_train).select(\"category\", \"words\")\n",
    "\n",
    "\n",
    "tokenized_train.show(n=5, truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_bAd-Lr2Dff",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "filtered_train = remover.transform(tokenized_train).select(\"category\", \"filtered\")\n",
    "\n",
    "filtered_train.show(n=5, truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "APSY4QlHBaDK",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# convert feature to vector\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "vectorizer = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=5000, minDF=2.0)\n",
    "\n",
    "cv_model = vectorizer.fit(filtered_train)\n",
    "\n",
    "train_feature = cv_model.transform(filtered_train).select(\"category\",\"features\")\n",
    "train_feature.show(n=5, truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkoXgUfFkXnX",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# convert label to index\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "\n",
    "index_model = indexer.fit(train_feature)\n",
    "\n",
    "train_xy = index_model.transform(train_feature).select(\"features\", \"label\")\n",
    "\n",
    "train_xy.show(n=5, truncate=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KLcZDsxLmJFe",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# preprocess test data\n",
    "\n",
    "test_data = spark.read.csv(test_file, schema=data_schema, sep='\\t', header=None)\n",
    "\n",
    "lower_test = test_data.select('category', lower(col('text')).alias('text'))\n",
    "\n",
    "tokenized_test = tokenizer.transform(lower_test).select(\"category\", \"words\")\n",
    "\n",
    "filtered_test = remover.transform(tokenized_test).select(\"category\", \"filtered\")\n",
    "\n",
    "test_feature = cv_model.transform(filtered_test).select(\"category\",\"features\")\n",
    "\n",
    "test_xy = index_model.transform(test_feature).select(\"features\", \"label\")\n",
    "\n",
    "test_xy.show(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nVm-u5UbnWyJ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "\n",
    "cls = LogisticRegression()\n",
    "\n",
    "lrModel = cls.fit(train_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q0u_jgKVqEGw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(test_xy)\n",
    "\n",
    "predictions.show(n=5, truncate=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z270xWF-pS7q",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate AUC\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "\n",
    "evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "foPFuhUxqM_h",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "\n",
    "print(f\"accuracy: {metrics.accuracy}\")\n",
    "print(f\"precision: {metrics.precision(1.0)}\")\n",
    "\n",
    "print(f\"recall: {metrics.recall(1.0)}\")\n",
    "\n",
    "print(f\"f1 score: {metrics.fMeasure(1.0, 1.0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JfG7oIZfwXTx",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# data is imbalanced\n",
    "\n",
    "train_label_count = train_xy.groupby('label').count().toPandas()\n",
    "train_label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ISz-8RYer2qg",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# add weight to handle imbalanced classes\n",
    "\n",
    "ratio = train_label_count.loc[0, 'count'] / train_label_count.loc[1, 'count']\n",
    "\n",
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Md3UNhAxdFz",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "def weight_balance(labels):\n",
    "    return when(labels == 1, ratio).otherwise(1)\n",
    "\n",
    "train_xy_weight = train_xy.withColumn('weights', weight_balance(col('label')))\n",
    "\n",
    "train_xy_weight.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zufm_zLryZhS",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cls = LogisticRegression(weightCol=\"weights\")\n",
    "\n",
    "lrModel = cls.fit(train_xy_weight)\n",
    "\n",
    "predictions = lrModel.transform(test_xy)\n",
    "\n",
    "predictions.show(n=5, truncate=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tpQ8FGIMy8gE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "\n",
    "print(f\"accuracy: {metrics.accuracy}\")\n",
    "print(f\"precision: {metrics.precision(1.0)}\")\n",
    "\n",
    "print(f\"recall: {metrics.recall(1.0)}\")\n",
    "\n",
    "print(f\"f1 score: {metrics.fMeasure(1.0, 1.0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DfdNLwAMvhho",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "try RegexTokenizer instead of Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1eVeU1JDzD01",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# stop\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "pyspark_nlp_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
