{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark_nlp_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMUDOU4R0w6g",
        "colab_type": "text"
      },
      "source": [
        "# PySpark NLP Tutorial\n",
        "\n",
        "Upload this jupyter notebook to Google drive, and open this tutorial with Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9kZ5EW9qsxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install the dependencies:\n",
        "%env spark_version=2.4.4\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-${spark_version}/spark-${spark_version}-bin-hadoop2.7.tgz\n",
        "!tar xf spark-${spark_version}-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!python --version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZlOhOZGreFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set environment\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-{}-bin-hadoop2.7\".format(os.environ[\"spark_version\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkBiywZk4_Lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat /etc/spark-defaults.conf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qCjjRcyuJqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install pyspark\n",
        "!pip install pyspark\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0SU5vJSteSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init() # os.environ[\"SPARK_HOME\"]\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF-3SzXQ_Yji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download data\n",
        "import requests\n",
        "\n",
        "url = \"https://github.com/liuhoward/teaching/raw/master/big_data/smsspam/\"\n",
        "train_file = \"SMSSpamCollection.train\"\n",
        "test_file = \"SMSSpamCollection.test\"\n",
        "\n",
        "r = requests.get(url + train_file)\n",
        "open(train_file, 'wb').write(r.content)\n",
        "\n",
        "r = requests.get(url + test_file)\n",
        "open(test_file, 'wb').write(r.content)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEY5lH8NthSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load train data\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# define schema\n",
        "data_schema = StructType([\n",
        "    StructField(\"category\", StringType()),\n",
        "    StructField(\"text\", StringType())])\n",
        "\n",
        "# read train csv file\n",
        "train_data = spark.read.csv(train_file, schema=data_schema, sep='\\t', header=None)\n",
        "print(type(train_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6YfHbtaxpU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data.printSchema() # print detail schema of data\n",
        "train_data.show(n=5, truncate=False) # show top 5 rows\n",
        "train_data.count()  # number of examples\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1qdZ3zgAab2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lower case\n",
        "\n",
        "from pyspark.sql.functions import lower, col\n",
        "\n",
        "lower_train = train_data.select('category', lower(col('text')).alias('text'))\n",
        "\n",
        "lower_train.show(n=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzSMr3Bt1oOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize\n",
        "\n",
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from pyspark.sql.functions import col, udf\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "\n",
        "tokenized_train = tokenizer.transform(lower_train).select(\"category\", \"words\")\n",
        "\n",
        "\n",
        "tokenized_train.show(n=5, truncate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_bAd-Lr2Dff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove stopwords\n",
        "\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "filtered_train = remover.transform(tokenized_train).select(\"category\", \"filtered\")\n",
        "\n",
        "filtered_train.show(n=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APSY4QlHBaDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert feature to vector\n",
        "\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "# fit a CountVectorizerModel from the corpus.\n",
        "vectorizer = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5.0)\n",
        "\n",
        "cv_model = vectorizer.fit(filtered_train)\n",
        "\n",
        "train_feature = cv_model.transform(filtered_train).select(\"category\",\"features\")\n",
        "train_feature.show(n=5, truncate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkoXgUfFkXnX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert label to index\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
        "\n",
        "index_model = indexer.fit(train_feature)\n",
        "\n",
        "train_xy = index_model.transform(train_feature).select(\"features\", \"label\")\n",
        "\n",
        "train_xy.show(n=5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLcZDsxLmJFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocess test data\n",
        "\n",
        "test_data = spark.read.csv(test_file, schema=data_schema, sep='\\t', header=None)\n",
        "\n",
        "lower_test = test_data.select('category', lower(col('text')).alias('text'))\n",
        "\n",
        "tokenized_test = tokenizer.transform(lower_test).select(\"category\", \"words\")\n",
        "\n",
        "filtered_test = remover.transform(tokenized_test).select(\"category\", \"filtered\")\n",
        "\n",
        "test_feature = cv_model.transform(filtered_test).select(\"category\",\"features\")\n",
        "\n",
        "test_xy = index_model.transform(test_feature).select(\"features\", \"label\")\n",
        "\n",
        "test_xy.show(n=5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVm-u5UbnWyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "\n",
        "cls = LogisticRegression()\n",
        "\n",
        "lrModel = cls.fit(train_xy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0u_jgKVqEGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = lrModel.transform(test_xy)\n",
        "\n",
        "predictions.show(n=5, truncate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z270xWF-pS7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate AUC\n",
        "\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
        "\n",
        "evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foPFuhUxqM_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate\n",
        "\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "metrics = MulticlassMetrics(predictionAndLabels)\n",
        "\n",
        "\n",
        "print(f\"accuracy: {metrics.accuracy}\")\n",
        "print(f\"precision: {metrics.precision(1.0)}\")\n",
        "\n",
        "print(f\"recall: {metrics.recall(1.0)}\")\n",
        "\n",
        "print(f\"f1 score: {metrics.fMeasure(1.0, 1.0)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfG7oIZfwXTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data is imbalanced\n",
        "\n",
        "train_label_count = train_xy.groupby('label').count().toPandas()\n",
        "train_label_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISz-8RYer2qg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add weight to handle imbalanced classes\n",
        "\n",
        "ratio = train_label_count.loc[0, 'count'] / train_label_count.loc[1, 'count']\n",
        "\n",
        "ratio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Md3UNhAxdFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "def weight_balance(labels):\n",
        "    return when(labels == 1, ratio).otherwise(1)\n",
        "\n",
        "train_xy_weight = train_xy.withColumn('weights', weight_balance(col('label')))\n",
        "\n",
        "train_xy_weight.show(n=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zufm_zLryZhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cls = LogisticRegression(weightCol=\"weights\")\n",
        "\n",
        "lrModel = cls.fit(train_xy_weight)\n",
        "\n",
        "predictions = lrModel.transform(test_xy)\n",
        "\n",
        "predictions.show(n=5, truncate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpQ8FGIMy8gE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "metrics = MulticlassMetrics(predictionAndLabels)\n",
        "\n",
        "\n",
        "print(f\"accuracy: {metrics.accuracy}\")\n",
        "print(f\"precision: {metrics.precision(1.0)}\")\n",
        "\n",
        "print(f\"recall: {metrics.recall(1.0)}\")\n",
        "\n",
        "print(f\"f1 score: {metrics.fMeasure(1.0, 1.0)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eVeU1JDzD01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}